# -*- coding: utf-8 -*-
"""Cleaned_bigdata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IbUDG7TmMCox5POC-oLTApnDimGJZmXy
"""

"""

## Original Code written for SBU's Big Data Analytics Project 
##
## Student Name: Sai Bhargav Varanasi
## Student ID: 114707860
## Student Name: Aniket Panda
## Student ID: 114356301
## Student Name: Akash Sateesh
## Student ID: 113221752
## Student Name: Priyanka Amol Dighe
## Student ID: 113264191

"""

from google.colab import drive
drive.mount("/content/drive/", force_remount= True)

cd 'drive/My Drive'

cd 'bigdata/'

!pip3 install pickle5
import pickle5 as pickle
import pandas as pd

with open('./shingleDF.pkl', "rb") as fh:
  df = pickle.load(fh)
with open('./shingleDF_18.pkl', "rb") as f:
  test_df = pickle.load(f)

import numpy as np

# cleaning data- replace empty with nan
df = df.fillna(value=np.nan)
#making new class no disaster for none 
df["disasterType"].replace({"": "NoDisaster"}, inplace=True)

# if data type of float and nan, fill with median
for column in df.columns:
  if df[column].dtype.name == 'float64':
    df[column].fillna(df[column].median(), inplace=True)


# FOR TEST DATA
# cleaning data- replace empty with nan 
test_df = test_df.fillna(value=np.nan)
#making new class no disaster for none 
test_df["disasterType"].replace({"": "NoDisaster"}, inplace=True)

# if data type of float and nan, fill with median
for column in test_df.columns:
  if test_df[column].dtype.name == 'float64':
    test_df[column].fillna(test_df[column].median(), inplace=True)

#converting disaster type column to category and dropping unrequired disasters

df['disasterType'] = df.disasterType.astype('category')
df.disasterType.dtype.name
df.dropna()

test_df['disasterType'] = test_df.disasterType.astype('category')
test_df.disasterType.dtype.name
test_df.dropna()

#define values
values = ['Toxic Substances', 'Mud/Landslide', 'Chemical', 'Earthquake' , 'Terrorist',  'Other', 'Snow', 'Tornado']

#drop rows that contain any value in the list
df = df[df.disasterType.isin(values) == False]
test_df = test_df[test_df.disasterType.isin(values) == False]

test_df.disasterType.unique()

df.disasterType.unique()

# one - hot encoding disasterType columns
disasterList = df.disasterType.unique()

for disaster in disasterList:
  df[disaster] = df.apply (lambda row: 1 if row.disasterType == disaster else 0 , axis=1)
  # df.top_pickle()
# df = pickle.load('one_hot_encoded_5year.pkl')



# FOR test data
disasterList = test_df.disasterType.unique()

for disaster in disasterList:
  test_df[disaster] = test_df.apply (lambda row: 1 if row.disasterType == disaster else 0 , axis=1)

# normalize data for training

def normalize(df):
    result = df.copy()
    for feature_name in df.columns:
      if df[feature_name].dtype.name == 'float64':
        max_value = df[feature_name].max()
        min_value = df[feature_name].min()
        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)
    return result
    
normalized_df = normalize(df)

normalized_test_df = normalize(test_df)

#Pickling Files

df.to_pickle("final_df.pkl")
test_df.to_pickle("final_test_df.pkl")
normalized_df.to_pickle("normalized_final_df.pkl")
normalized_test_df.to_pickle("normalized_final_test_df.pkl")

print(df["Hurricane"].value_counts())

#importing pickle files

import pandas as pd

df = pd.read_pickle("final_df.pkl")
test_df= pd.read_pickle("final_test_df.pkl")
normalized_df= pd.read_pickle("normalized_final_df.pkl")
normalized_test_df= pd.read_pickle("normalized_final_test_df.pkl")

test_df.columns

# finding columns for training
unrequired = set(['location','startDate','endDate','NoDisaster','disasterType', 'Flood', 'Snow', 'Hurricane',
       'Severe Ice Storm', 'Severe Storm(s)', 'Fire', 'Tornado'])
columns = set(df.columns)
train_columns  = sorted(list(columns - unrequired))

print(train_columns)

# Training initial Logistic regression model to verify proof of concept

from tensorflow.python.ops.gen_nn_ops import DataFormatDimMap
from sklearn.model_selection import train_test_split
from sklearn.utils import resample

from sklearn import preprocessing

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_regression
from sklearn.datasets import load_boston
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc
from sklearn import metrics


# for label in labels:
label = 'Hurricane'

X = normalized_df[train_columns]
y = df[label]

select = SelectKBest(score_func=chi2, k=16)
select.fit_transform(X,y)

cols = select.get_support(indices=True)
required_cols = [train_columns[i] for i in cols]
features_df_new = df[required_cols]
print(features_df_new.columns)

X = features_df_new

#split data into test and training sets
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=27)

# combine them back for resampling
combined_data = pd.concat([X_train, y_train], axis=1)

Ns = combined_data[combined_data[label] == 0]
Ys = combined_data[combined_data[label] == 1]

pos_upsampled = resample(Ns,
 replace=True, # sample with replacement
 n_samples=len(Ys), # match number in majority class
 random_state=27)

# combine majority and upsampled minority
upsampled = pd.concat([Ys, pos_upsampled])

X_train = upsampled[required_cols]
y_train = upsampled[label]

print(X_train.columns)

#using SMOTE to generate synthetic data for making number of positives and negatives equal
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)
# sm = SMOTE(random_state=27, sampling_strategy=1.0)
# X_train, y_train = sm.fit_resample(X_train, y_train)

# y_train = pd.DataFrame(y_train, columns = [label])
fitTry = LogisticRegression()

# fit the model with data
fitTry.fit(X_train, y_train)

prediction=fitTry.predict(X_test)

cnf_matrix = metrics.confusion_matrix(y_test, prediction)

falsePositives, actualPositives, thresholds = roc_curve(y_test, prediction)

print("Area Under Curve:", auc(falsePositives, actualPositives))
print("Accuracy:",metrics.accuracy_score(y_test, prediction))
print("Error Rate:", 1-metrics.accuracy_score(y_test, prediction))

# training 5-layer binary classifier keras model per disaster class

from tensorflow.python.ops.gen_nn_ops import DataFormatDimMap
from sklearn.model_selection import train_test_split
from sklearn.utils import resample

from sklearn import preprocessing

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_regression
from sklearn.datasets import load_boston
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc
from sklearn import metrics
import tensorflow as tf
from tensorflow import keras


model_dict = dict()
best_params = dict()
loss_accuracy = dict()



uniques = list(df.disasterType.unique())

for label in uniques:
  X = normalized_df[train_columns]
  y = df[label]

  select = SelectKBest(score_func=chi2, k=16)
  select.fit_transform(X,y)

  cols = select.get_support(indices=True)
  required_cols = [train_columns[i] for i in cols]

  #adding best params
  best_params[label] = required_cols

  features_df_new = df[required_cols]
  print(label, features_df_new.columns)

  X = features_df_new

  #split data into test and training sets
  X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=27)


  # combine them back for resampling
  combined_data = pd.concat([X_train, y_train], axis=1)

  Ns = combined_data[combined_data[label] == 0]
  Ys = combined_data[combined_data[label] == 1]

  pos_upsampled = resample(Ns,
  replace=True, # sample with replacement
  n_samples=len(Ys), # match number in majority class
  random_state=27)



  # combine majority and upsampled minority
  upsampled = pd.concat([Ys, pos_upsampled])

  X_train = upsampled[required_cols]
  y_train = upsampled[label]


  model = keras.Sequential([
      keras.layers.Flatten(input_shape=(16,)),
      keras.layers.Dense(16, activation=tf.nn.relu),
      keras.layers.Dense(16, activation=tf.nn.relu),
      keras.layers.Dense(16, activation=tf.nn.relu),
      keras.layers.Dense(1, activation=tf.nn.sigmoid),
  ])


  model.compile(optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy'])

  model.fit(X_train, y_train, epochs=25, batch_size=8)
  test_loss, test_acc = model.evaluate(X_test, y_test)

  loss_accuracy[label] = ("loss = "+ str(test_loss), "accuracy = "+ str(test_acc))
  model_dict[label] = model
  
print(best_params)
print(model_dict)
print(loss_accuracy)



#Pickling dictionaries of models 

import pickle

with open ('disasters_models.pkl', 'wb') as f:
  pickle.dump(model_dict, f)

with open ('best_params.pkl', 'wb') as f:
  pickle.dump(best_params, f)



#importing dictionaries of models 

import pickle

with open('no_disasters_models.pkl', 'rb') as f:
  no_disaster_model = pickle.load(f)

with open('disasters_models.pkl', 'rb') as f:
  model_dict = pickle.load(f)

with open ('best_params.pkl', 'rb') as f:
  best_params = pickle.load(f)

model_dict.update(no_disaster_model)

print(model_dict)

from sklearn import metrics



#getting predictions from each individual model and generating metrics

predictions_dict = dict()
cnf_matrix_dict = dict()

disasters_in_2018  = set(test_df.disasterType.unique())
test_disaster_set = set(model_dict.keys()).intersection(disasters_in_2018)

print(test_disaster_set)

for key in model_dict.copy().keys():
  if key not in test_disaster_set:
    model_dict.pop(key)

for label in model_dict.keys():

  model = model_dict[label]

  X_predict = test_df[best_params[label]]
  y_predict = test_df[label]

  curr_model_prediction = model.predict(X_predict)
  predictions_dict[label] = curr_model_prediction

  zero_one_predictions = []
  for val in curr_model_prediction:
    if 1- val > val :
      zero_one_predictions.append(0)
    else:
      zero_one_predictions.append(1)

  curr_cnf_matrix = metrics.confusion_matrix(y_predict, zero_one_predictions)

  cnf_matrix_dict[label] = curr_cnf_matrix


print(cnf_matrix_dict)
print(predictions_dict)



#Getting scores for each model prediction and doing max to find prediction

from re import I
final_label_predictions = []
final_max_probability = []

for i in range(len(test_df.index)):
  max = float('-inf')
  label = ''
  for key in predictions_dict.keys():
    if predictions_dict[key][i] > max:
      max = predictions_dict[key][i]
      label = key
  final_max_probability.append(max)
  final_label_predictions.append(label)


print(final_label_predictions)

#merging project 

test_df['label_predictions'] = final_label_predictions
test_df['prediction_probability'] = final_max_probability

#getting results and processing 

test_df[['County','State',]]=test_df.location.str.split(',',expand=True)

test_df.iloc[0]

print( test_df[(test_df.State == "TX") & (test_df.disasterType == "Flood")].count())

cali_df = test_df[(test_df.State == "SC") & (test_df.label_predictions == "Hurricane")]
cali_df.sort_values('startDate')

sorted(list(cali_df['County'].unique()), reverse = True)

curr_cnf_matrix = metrics.multilabel_confusion_matrix(test_df['disasterType'], test_df['label_predictions'])
print(curr_cnf_matrix)

test_df.label_predictions.value_counts()

print( metrics.classification_report(test_df['disasterType'],test_df['label_predictions']))
